{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LGoUAFQDHuGE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5f1c21c-d8d8-4d4b-a7ea-5138b665421f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting GPUtil\n",
            "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: GPUtil\n",
            "  Building wheel for GPUtil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7393 sha256=ecaa7e09b01c069f079504bdb40af4a84bae6c33653670592af0c075dfb8d4ff\n",
            "  Stored in directory: /root/.cache/pip/wheels/a9/8a/bd/81082387151853ab8b6b3ef33426e98f5cbfebc3c397a9d4d0\n",
            "Successfully built GPUtil\n",
            "Installing collected packages: GPUtil\n",
            "Successfully installed GPUtil-1.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install GPUtil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HnDJHS5mH81o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a4af6c4-f723-45c5-f70f-390fe83ee32f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m81.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.29.2\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvjB6f0LIOkb"
      },
      "outputs": [],
      "source": [
        "!pip install pytorch_pretrained_bert googletrans"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyarabic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uktVnlu-5aPH",
        "outputId": "6c11a9e6-d577-4a4f-cac9-b076858b8f81"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyarabic\n",
            "  Downloading PyArabic-0.6.15-py3-none-any.whl (126 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.4/126.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from pyarabic) (1.16.0)\n",
            "Installing collected packages: pyarabic\n",
            "Successfully installed pyarabic-0.6.15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "GJow2fCA-ECG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce287c25-930f-4e20-f01e-02e7461c86c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import json, sys, regex\n",
        "import torch\n",
        "import GPUtil\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertConfig, BertAdam, BertForSequenceClassification\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, classification_report, confusion_matrix\n",
        "##----------------------------------------------------\n",
        "from transformers import *\n",
        "from transformers import XLMRobertaConfig\n",
        "from transformers import XLMRobertaModel\n",
        "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
        "from transformers import XLMRobertaForSequenceClassification, XLMRobertaTokenizer, XLMRobertaModel\n",
        "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from preprocessing import preprocess\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXd8ERH0OMDH"
      },
      "outputs": [],
      "source": [
        "# Load the MARBERT tokenizer\n",
        "MARBERT_tokenizer = AutoTokenizer.from_pretrained(\"UBC-NLP/MARBERT\")\n",
        "\n",
        "# Load the MARBERT model for multi-label classification\n",
        "MARBERT_model = AutoModelForSequenceClassification.from_pretrained(\"UBC-NLP/MARBERT\", num_labels=11)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dn_4MB0h5q3e",
        "outputId": "1c53dfe9-c5c7-428e-c0c2-f63470217d13"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "wIwgjQoJPb6t"
      },
      "outputs": [],
      "source": [
        "dataset_train_raw = preprocess(\"2018-E-c-Ar-train.xlsx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Sc0J17NhQTY7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e61ff1f-d54b-46f9-9a85-c29cd2865598"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        }
      ],
      "source": [
        "# Preprocess your dataset\n",
        "texts_train = dataset_train_raw[\"Tweet\"].tolist()\n",
        "\n",
        "encoded_inputs_train = MARBERT_tokenizer(texts_train, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "labels_train = dataset_train_raw.iloc[:, 2:].values  # Labels start from the third column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "_O02IBKXOVmv"
      },
      "outputs": [],
      "source": [
        "# Convert the labels to tensors\n",
        "labels_train = torch.tensor(labels_train, dtype=torch.float)\n",
        "\n",
        "# Create a TensorDataset from the encoded inputs and labels\n",
        "dataset_train = TensorDataset(encoded_inputs_train.input_ids, encoded_inputs_train.attention_mask, labels_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gK6YKeAoQoFC",
        "outputId": "99a4cf82-ab6c-4cbc-d101-bd4e76a6a773"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of dataset_train: 2278\n",
            "Shape of input_ids tensor in dataset_train: torch.Size([2278, 41])\n",
            "Shape of attention_mask tensor in dataset_train: torch.Size([2278, 41])\n",
            "Shape of labels tensor in dataset_train: torch.Size([2278, 11])\n"
          ]
        }
      ],
      "source": [
        "print(\"Length of dataset_train:\", len(dataset_train))\n",
        "print(\"Shape of input_ids tensor in dataset_train:\", dataset_train.tensors[0].shape)\n",
        "print(\"Shape of attention_mask tensor in dataset_train:\", dataset_train.tensors[1].shape)\n",
        "print(\"Shape of labels tensor in dataset_train:\", dataset_train.tensors[2].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "VkYBYabWRBPF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d3dec5c-433a-4d37-baae-ff722ed600b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Define hyperparameters and optimizer\n",
        "batch_size = 16\n",
        "num_epochs = 4\n",
        "learning_rate = 1e-4\n",
        "optimizer = AdamW(MARBERT_model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "Qs-SxvyqRH27"
      },
      "outputs": [],
      "source": [
        "# Create a DataLoader for the training dataset\n",
        "dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPCGe_1dOH8-",
        "outputId": "e41c24dc-8a1b-4077-f12e-e939da811507"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4 - Average Loss: 0.1694\n",
            "Epoch 2/4 - Average Loss: 0.1812\n",
            "Epoch 3/4 - Average Loss: 0.1379\n",
            "Epoch 4/4 - Average Loss: 0.0915\n"
          ]
        }
      ],
      "source": [
        "# Perform training\n",
        "MARBERT_model.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    \n",
        "    for batch in dataloader_train:\n",
        "        # Retrieve the batch inputs and labels\n",
        "        input_ids, attention_mask, batch_labels = batch\n",
        "        \n",
        "        # Clear gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = MARBERT_model(input_ids=input_ids, attention_mask=attention_mask, labels=batch_labels)\n",
        "        loss = outputs.loss\n",
        "        \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Update total loss\n",
        "        total_loss += loss.item()\n",
        "    \n",
        "    # Print average loss for the epoch\n",
        "    average_loss = total_loss / len(dataloader_train)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Average Loss: {average_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "JK6Srr1XzqDf"
      },
      "outputs": [],
      "source": [
        "model_save_name = 'marbert_4epochs.pt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "E1Ioe4eiwhJY"
      },
      "outputs": [],
      "source": [
        "# Saving the trained model\n",
        "torch.save(MARBERT_model.state_dict(), model_save_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "prhTyO5uzc70",
        "outputId": "ff91fbf1-4a52-4a99-86b1-c93c85aea7a2"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_9156f14c-5094-4130-9aa4-8525c2db23e1\", \"checkpoint.pth\", 651482577)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "files.download('checkpoint.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96TwPYXL0u1i",
        "outputId": "6b38639a-ac04-4277-b066-015ce1311185"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load saved model\n",
        "MARBERT_model.load_state_dict(torch.load(path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "tiUROwSt3ZDZ"
      },
      "outputs": [],
      "source": [
        "dataset_test_raw = preprocess(\"2018-E-c-Ar-test-gold.xlsx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "IhUGfd0R4OAH"
      },
      "outputs": [],
      "source": [
        "# Preprocess your dataset\n",
        "texts_test = dataset_test_raw[\"Tweet\"].tolist()\n",
        "\n",
        "encoded_inputs_test = MARBERT_tokenizer(texts_test, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "labels_test = dataset_test_raw.iloc[:, 2:].values  # Assuming labels start from the third column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "k-McWApn4Z3g"
      },
      "outputs": [],
      "source": [
        "# Convert the labels to tensors\n",
        "labels_test = torch.tensor(labels_test, dtype=torch.float)\n",
        "\n",
        "# Create a TensorDataset from the encoded inputs and labels\n",
        "dataset_test = TensorDataset(encoded_inputs_test.input_ids, encoded_inputs_test.attention_mask, labels_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "h-kXdZzj4mNM"
      },
      "outputs": [],
      "source": [
        "# Create a DataLoader for the test dataset\n",
        "\n",
        "dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZLiPewq5Cwo",
        "outputId": "9bdcb73f-2539-4970-ddd5-2f0fd042201f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.2688\n",
            "Precision: 0.7205\n",
            "Recall: 0.6513\n",
            "F1-Score: 0.6712\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Lists to store predicted and true labels\n",
        "all_predictions = []\n",
        "all_true_labels = []\n",
        "\n",
        "# Disable gradient calculation\n",
        "with torch.no_grad():\n",
        "    # Set the model to evaluation mode\n",
        "    MARBERT_model.eval()\n",
        "\n",
        "    # Iterate over the test dataloader\n",
        "    for batch in dataloader_test:\n",
        "        # Retrieve the batch inputs and labels\n",
        "        input_ids, attention_mask, batch_labels = batch\n",
        "\n",
        "        # Move the batch tensors to the device\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        batch_labels = batch_labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = MARBERT_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Convert logits to probabilities\n",
        "        probabilities = torch.sigmoid(logits)\n",
        "\n",
        "        # Convert probabilities to predicted labels (0 or 1)\n",
        "        predictions = torch.round(probabilities)\n",
        "\n",
        "        # Append predictions and true labels to the lists\n",
        "        all_predictions.extend(predictions.cpu().numpy())\n",
        "        all_true_labels.extend(batch_labels.cpu().numpy())\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(all_true_labels, all_predictions)\n",
        "precision = precision_score(all_true_labels, all_predictions, average='weighted')\n",
        "recall = recall_score(all_true_labels, all_predictions, average='weighted')\n",
        "f1 = f1_score(all_true_labels, all_predictions, average='weighted')\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWVQDTVPJW0B"
      },
      "outputs": [],
      "source": [
        "# Looping on all test sentences and showing the output probabilities.\n",
        "# Set the device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Specify the output file path\n",
        "output_file = \"predicted_probabilities.txt\"\n",
        "\n",
        "test_sentences = dataset_test_raw[\"Tweet\"].tolist()\n",
        "\n",
        "# Open the output file in write mode\n",
        "with open(output_file, \"w\") as file:\n",
        "    for sentence in test_sentences:\n",
        "        file.write(\"******************************************************************\\n\")\n",
        "        file.write(f\"________________{sentence}____________________\\n\")\n",
        "\n",
        "        # Tokenize the sentence\n",
        "        encoded_input = MARBERT_tokenizer.encode_plus(\n",
        "            sentence,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Move the input tensors to the device\n",
        "        input_ids = encoded_input[\"input_ids\"].to(device)\n",
        "        attention_mask = encoded_input[\"attention_mask\"].to(device)\n",
        "\n",
        "        # Set the model to evaluation mode\n",
        "        MARBERT_model.eval()\n",
        "\n",
        "        # Move the model to the device\n",
        "        MARBERT_model.to(device)\n",
        "\n",
        "        # Disable gradient calculation\n",
        "        with torch.no_grad():\n",
        "            # Forward pass\n",
        "            outputs = MARBERT_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "\n",
        "        # Convert logits to probabilities\n",
        "        probabilities = torch.sigmoid(logits)\n",
        "\n",
        "        label_names = [\"anger\",\"anticipation\",\"disgust\",\"fear\",\"joy\",\"love\",\"optimism\",\"pessimism\",\"sadness\",\"surprise\",\"trust\"]  \n",
        "\n",
        "        # Write the predicted probabilities for each label to the file\n",
        "        for i, label_name in enumerate(label_names):\n",
        "            file.write(f\"{label_name}: {probabilities[0][i].item():.4f}\\n\")\n",
        "            \n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.13 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "88033ea6e6aaf353f3d26ef69434bb9b1f089d6b00d896155ae24c39a5d92896"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}